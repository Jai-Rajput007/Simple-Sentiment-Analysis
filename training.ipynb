{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d8dc9fe104f99bec80e544e03be15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JAY SINGH RAJPUT\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36304952c75429dbba2240487af9956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90722988f7c436eae5b5e2f00b9ac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8167a8127dc449498d4ce8864314ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cd51db5a1044d5b6db2ab4e5af9d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66012322ed724dfb9c2ddc1cd2a2b9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9c8f48d4d543f9b6265e1304879db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon Polarity dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6d208f9a1340949a81b51ade5b8df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JAY SINGH RAJPUT\\.cache\\huggingface\\hub\\datasets--amazon_polarity. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db2efaf444e4cec838a60a492eaf6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00004.parquet:   0%|          | 0.00/260M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a4237f8d414a3b9b04f91973ebb072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00004.parquet:   0%|          | 0.00/258M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6ec51ef08f4f15801a2f1f9b8a4120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00004.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6dbcd800ca452f9ed4926579ea5aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00004.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a826593e8ecc44cfb3f4a6ba77ec292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc59242365640199dcdbea7becd782c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b68786daf0f433e8136af7ef5e9d29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 1) Load datasets: IMDB (movie reviews) and Amazon Polarity (product reviews)\n",
    "print(\"Loading IMDB dataset...\")\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(\"Loading Amazon Polarity dataset...\")\n",
    "amazon_dataset = load_dataset(\"amazon_polarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrames\n",
    "imdb_train_df = pd.DataFrame({'text': imdb_dataset['train']['text'], 'label': imdb_dataset['train']['label']})\n",
    "imdb_test_df = pd.DataFrame({'text': imdb_dataset['test']['text'], 'label': imdb_dataset['test']['label']})\n",
    "\n",
    "amazon_train_df = pd.DataFrame({'text': amazon_dataset['train']['content'], 'label': amazon_dataset['train']['label']})\n",
    "amazon_test_df = pd.DataFrame({'text': amazon_dataset['test']['content'], 'label': amazon_dataset['test']['label']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the datasets (smaller subset for 8GB RAM)\n",
    "print(\"Combining datasets...\")\n",
    "train_df = pd.concat([\n",
    "    imdb_train_df.sample(2500, random_state=42),  # Subset of 2,500 IMDB reviews\n",
    "    amazon_train_df.sample(2500, random_state=42)  # Subset of 2,500 Amazon reviews\n",
    "], ignore_index=True)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    imdb_test_df.sample(1000, random_state=42),  # Subset of 1,000 IMDB reviews\n",
    "    amazon_test_df.sample(1000, random_state=42)  # Subset of 1,000 Amazon reviews\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Load pre-trained GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "embedding_dim = 300  # Using 300-dimensional GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Function to preprocess text and convert to embeddings\n",
    "def text_to_embedding(text, embeddings_index, embedding_dim):\n",
    "    # Simple preprocessing: lowercase, remove punctuation, split into words\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    words = text.split()\n",
    "    \n",
    "    # Get embeddings for each word and average them\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in embeddings_index:\n",
    "            embeddings.append(embeddings_index[word])\n",
    "    \n",
    "    # If no words found in embeddings, return a zero vector\n",
    "    if not embeddings:\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    # Average the embeddings\n",
    "    embeddings = np.array(embeddings)\n",
    "    return np.mean(embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training texts to embeddings...\n",
      "Converting test texts to embeddings...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Convert all texts to embeddings\n",
    "print(\"Converting training texts to embeddings...\")\n",
    "X_train = np.array([text_to_embedding(text, embeddings_index, embedding_dim) for text in train_df['text']])\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "print(\"Converting test texts to embeddings...\")\n",
    "X_test = np.array([text_to_embedding(text, embeddings_index, embedding_dim) for text in test_df['text']])\n",
    "y_test = test_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Define a simple neural network for sentiment classification\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Train the model\n",
    "input_dim = embedding_dim  # 300 (from GloVe embeddings)\n",
    "hidden_dim = 128  # Smaller hidden layer\n",
    "model = SentimentClassifier(input_dim, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/10, Loss: 0.5363, Test Accuracy: 0.7020\n",
      "Epoch 2/10, Loss: 0.4672, Test Accuracy: 0.7430\n",
      "Epoch 3/10, Loss: 0.4033, Test Accuracy: 0.7740\n",
      "Epoch 4/10, Loss: 0.3463, Test Accuracy: 0.7775\n",
      "Epoch 5/10, Loss: 0.3014, Test Accuracy: 0.7790\n",
      "Epoch 6/10, Loss: 0.2692, Test Accuracy: 0.7825\n",
      "Epoch 7/10, Loss: 0.2426, Test Accuracy: 0.7870\n",
      "Epoch 8/10, Loss: 0.2207, Test Accuracy: 0.7885\n",
      "Epoch 9/10, Loss: 0.2021, Test Accuracy: 0.7885\n",
      "Epoch 10/10, Loss: 0.1861, Test Accuracy: 0.7880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64  # Smaller batch size to reduce memory usage\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_preds = (test_outputs >= 0.5).float()\n",
    "        accuracy = accuracy_score(y_test, test_preds.numpy())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7) Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_preds = (test_outputs >= 0.5).float().numpy()\n",
    "    test_probs = test_outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy:  0.7880\n",
      "Precision: 0.8144\n",
      "Recall:    0.7422\n",
      "F1-score:  0.7766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print performance metrics\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "precision = precision_score(y_test, test_preds)\n",
    "recall = recall_score(y_test, test_preds)\n",
    "f1 = f1_score(y_test, test_preds)\n",
    "print(\"\\nFinal Test Set Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) Function to predict sentiment on new text\n",
    "def predict_sentiment(text, model, embeddings_index, embedding_dim):\n",
    "    # Convert text to embedding\n",
    "    embedding = text_to_embedding(text, embeddings_index, embedding_dim)\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32).view(1, -1)\n",
    "    \n",
    "    # Predict sentiment\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(embedding_tensor)\n",
    "        prob = output.item()\n",
    "        pred = 1 if prob >= 0.5 else 0\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    return sentiment, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sentiment_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model on a variety of example texts:\n",
      "Text: I love this movie, it’s amazing!\n",
      "Sentiment: Positive, Probability: 0.9997\n",
      "\n",
      "Text: This phone is terrible, it keeps crashing.\n",
      "Sentiment: Negative, Probability: 0.0509\n",
      "\n",
      "Text: I had an amazing day at the park with my friends!\n",
      "Sentiment: Positive, Probability: 0.9989\n",
      "\n",
      "Text: The lecture was boring and unhelpful.\n",
      "Sentiment: Negative, Probability: 0.0032\n",
      "\n",
      "Text: I’m so excited for the weekend, it’s going to be great!\n",
      "Sentiment: Positive, Probability: 0.9881\n",
      "\n",
      "Text: The food at this restaurant was disappointing and overpriced.\n",
      "Sentiment: Negative, Probability: 0.0123\n",
      "\n",
      "Text: I really enjoyed the concert last night, the music was fantastic!\n",
      "Sentiment: Positive, Probability: 0.9993\n",
      "\n",
      "Text: My new laptop is super fast and easy to use.\n",
      "Sentiment: Positive, Probability: 0.9967\n",
      "\n",
      "Text: The weather today is awful, I hate this rain!\n",
      "Sentiment: Negative, Probability: 0.0724\n",
      "\n",
      "Text: I’m feeling so happy after talking to my best friend.\n",
      "Sentiment: Positive, Probability: 0.9795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9) Test on a variety of example texts\n",
    "example_texts = [\n",
    "    \"I love this movie, it’s amazing!\",  # Movie-related\n",
    "    \"This phone is terrible, it keeps crashing.\",  # Product review\n",
    "    \"I had an amazing day at the park with my friends!\",  # Casual talk\n",
    "    \"The lecture was boring and unhelpful.\",  # Feedback\n",
    "    \"I’m so excited for the weekend, it’s going to be great!\",  # Random thought\n",
    "    \"The food at this restaurant was disappointing and overpriced.\",  # Restaurant review\n",
    "    \"I really enjoyed the concert last night, the music was fantastic!\",  # Event\n",
    "    \"My new laptop is super fast and easy to use.\",  # Product review\n",
    "    \"The weather today is awful, I hate this rain!\",  # Weather\n",
    "    \"I’m feeling so happy after talking to my best friend.\"  # Emotion\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the model on a variety of example texts:\")\n",
    "for text in example_texts:\n",
    "    sentiment, prob = predict_sentiment(text, model, embeddings_index, embedding_dim)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}, Probability: {prob:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your own text to predict its sentiment (or type 'exit' to stop):\n",
      "Sentiment: Negative, Probability: 0.3934\n",
      "Sentiment: Negative, Probability: 0.3934\n",
      "Sentiment: Negative, Probability: 0.0937\n",
      "Sentiment: Positive, Probability: 0.9995\n",
      "Sentiment: Negative, Probability: 0.1302\n",
      "Sentiment: Negative, Probability: 0.3347\n",
      "Sentiment: Positive, Probability: 0.6121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10) Interactive sentiment prediction\n",
    "print(\"Enter your own text to predict its sentiment (or type 'exit' to stop):\")\n",
    "while True:\n",
    "    text = input(\"Text: \")\n",
    "    if text.lower() == 'exit':\n",
    "        break\n",
    "    sentiment, prob = predict_sentiment(text, model, embeddings_index, embedding_dim)\n",
    "    print(f\"Sentiment: {sentiment}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
